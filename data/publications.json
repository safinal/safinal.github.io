{
  "publications": [
    {
      "title": "Matina: A Culturally-Aligned Persian Language Model Using Multiple LoRA Experts",
      "journal": "ACL Findings",
      "year": "2025",
      "authors": [
        {
          "name": "S. Bourbour",
          "co": false
        },
        {
          "name": "M. SeifKashani",
          "co": false
        },
        {
          "name": "J. Seraj",
          "co": false
        },
        {
          "name": "F. Taherinezhad",
          "co": false
        },
        {
          "name": "A. Nafisi",
          "link": "https://scholar.google.com/citations?user=SM_UuoQAAAAJ&hl=en",
          "co": false
        },
        {
          "name": "F. Nadi",
          "co": false
        },
        {
          "name": "I. Barati",
          "co": false
        },
        {
          "name": "H. Hasani",
          "co": false
        },
        {
          "name": "M. Amiri",
          "co": false
        },
        {
          "name": "M. Masoudi",
          "co": false
        }
      ],
      "image": "/images/publications/matina.png",
      "url": "https://aclanthology.org/2025.findings-acl.1074/",
      "bibtex": {
        "data": {
          "@inproceedings": "hosseinbeigi-etal-2025-matina-culturally",
          "title": "Matina: A Culturally-Aligned {P}ersian Language Model Using Multiple {L}o{RA} Experts",
          "author": "Hosseinbeigi, Sara Bourbour  and  SeifKashani, MohammadAli  and  Seraj, Javad  and  Taherinezhad, Fatemeh  and  Nafisi, Ali  and  Nadi, Fatemeh  and  Barati, Iman  and  Hasani, Hosein  and  Amiri, Mostafa  and  Masoudi, Mostafa",
          "editor": "Che, Wanxiang  and  Nabende, Joyce  and  Shutova, Ekaterina  and  Pilehvar, Mohammad Taher",
          "booktitle": "Findings of the Association for Computational Linguistics: ACL 2025",
          "month": "jul",
          "year": "2025",
          "address": "Vienna, Austria",
          "publisher": "Association for Computational Linguistics",
          "url": "https://aclanthology.org/2025.findings-acl.1074/",
          "pages": "20874--20889",
          "ISBN": "979-8-89176-256-5",
          "abstract": "Large language models (LLMs) are powerful tools for a variety of applications, but to interact effectively with users, they must align with the cultural values and linguistic nuances of their audience. However, existing LLMs often fall short in adequately modeling underrepresented languages and cultures, such as Persian, limiting their applicability and acceptance. To address this, we construct diverse, high-quality datasets specifically tailored to Persian linguistic and cultural contexts, ensuring a more authentic and context-aware training process. Using these datasets, we develop Matina, a Persian-focused multi-expert model designed to embody Iranian cultural values and linguistic structures. Matina is trained by fine-tuning LLaMA3.1 8B-Instruct models across five domains: culinary, tourism, socio-culture, translation, and summarization. These experts are combined using a classifier to create a unified multi-expert system. By leveraging culturally aligned datasets, Matina outperforms baseline models in both task performance and user satisfaction, demonstrating the importance of data-driven cultural adaptation in LLM development."       
        }
      },
      "pdf": "https://aclanthology.org/2025.findings-acl.1074.pdf",
      "huggingFace": "https://huggingface.co/datasets/MatinaAI/instruction_tuning_datasets"
    },
    {
      "title": "Elastic deep autoencoder for text embedding clustering by an improved graph regularization",
      "journal": "Expert Systems with Applications",
      "year": "2024",
      "authors": [
        {
          "name": "F. Daneshfar",
          "co": false
        },
        {
          "name": "S. Soleymanbaigi",
          "co": false
        },
        {
          "name": "A. Nafisi",
          "link": "https://scholar.google.com/citations?user=SM_UuoQAAAAJ&hl=en",
          "co": false
        },
        {
          "name": "P. Yamini",
          "co": false
        }
      ],
      "image": "/images/publications/1-s2.0-S0957417423022820-gr3_lrg.jpg",
      "url": "https://doi.org/10.1016/j.eswa.2023.121780",
      "bibtex": {
        "data": {
          "@article": "DANESHFAR2024121780",
          "title": "Elastic deep autoencoder for text embedding clustering by an improved graph regularization",
          "Journal": "Expert Systems with Applications",
          "volume": "238",
          "pages": "121780",
          "year": "2024",
          "issn": "0957-4174",
          "doi": "https://doi.org/10.1016/j.eswa.2023.121780",
          "url": "https://www.sciencedirect.com/science/article/pii/S0957417423022820",
          "author": "Fatemeh Daneshfar and Sayvan Soleymanbaigi and Ali Nafisi and Pedram Yamini",
          "keywords": "Deep autoencoder, Text clustering, Graph regularization, Text embedding",
          "abstract": "Text clustering is a task for grouping extracted information of the text in different clusters, which has many applications in recommender systems, sentiment analysis, and more. Deep learning-based methods have become increasingly popular due to their high accuracy in identifying nonlinear structures. They usually consist of two major parts: dimensionality reduction and clustering. Autoencoders are simple unsupervised neural networks used for better representation of low-dimensional data and have shown good performance in dealing with non-linear features. However, while they utilize the Frobenius norm to deal well with Gaussian noise, they are sensitive to outlier data and Laplacian noise. In this paper, a deep autoencoder with an adapted elastic loss for text embedding clustering (EDA-TEC) is proposed. The elastic loss is a combination of the Frobenius norm and L2,1-norm to consider both types of noises. Additionally, to maintain the high-dimensional data geometric structure, a modified graph regularization term based on the weighted cosine similarity measure is used. EDA-TEC also improves clustering results by considering the sparsity regularization of the manifold representation data. In this jointly end-to-end deep learning model, better representation and text clustering results are achieved with high accuracy on common datasets compared to existing methods.11https://github.com/safinal/text-embedding-clustering."
        }
      },
      "pdf": "https://www.researchgate.net/publication/374150988_Elastic_Deep_Autoencoder_for_Text_Embedding_Clustering_by_an_Improved_Graph_Regularization?enrichId=rgreq-186b1411c3be642a97d3cf37f168ddb9-XXX&enrichSource=Y292ZXJQYWdlOzM3NDE1MDk4ODtBUzoxMTQzMTI4MTIwMDI3NzEwN0AxNjk3ODE0NDMyNTU1&el=1_x_2&_esc=publicationCoverPdf"
    }
  ]
}
